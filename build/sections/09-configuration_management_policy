<h1 id="9-configuration-management-policy">9. Configuration Management Policy</h1>

<p>Onyx standardizes and automates configuration management through the use of Chef/Salt scripts as well as documentation of all changes to production systems and networks. Chef and Salt automatically configure all Onyx systems according to established and tested policies, and are used as part of our Disaster Recovery plan and process.</p>

<h2 id="9-1-applicable-standards">9.1 Applicable Standards</h2>

<h3 id="9-1-1-applicable-standards-from-the-hitrust-common-security-framework">9.1.1 Applicable Standards from the HITRUST Common Security Framework</h3>

<ul>
<li>06 - Configuration Management</li>
</ul>

<h3 id="9-1-2-applicable-standards-from-the-hipaa-security-rule">9.1.2 Applicable Standards from the HIPAA Security Rule</h3>

<ul>
<li>164.310(a)(2)(iii) Access Control &amp; Validation Procedures</li>
</ul>

<h2 id="9-2-configuration-management-policies">9.2 Configuration Management Policies</h2>

<ol>
<li>Chef and Salt are used to standardize and automate configuration management.</li>
<li>No systems are deployed into Onyx environments without approval of the Onyx CTO.</li>
<li>All changes to production systems, network devices, and firewalls are approved by the Onyx CTO before they are implemented to assure they comply with business and security requirements.</li>
<li>All changes to production systems are tested before they are implemented in production.</li>
<li>Implementation of approved changes are only performed by authorized personnel.</li>
<li>Tooling to generate an up-to-date inventory of systems, including corresponding architecture diagrams for related products and services, is hosted on GitLab.

<ul>
<li>All systems are categorized as production and utility to differentiate based on criticality.</li>
<li>The Security Officer maintains scripts to generate inventory lists on demand using APIs provided by each cloud provider.</li>
<li>These scripts are used to generate the diagrams and asset lists required by the Risk Assessment phase of Onyx&rsquo;s Risk Management procedures (<a href="#4.3-risk-management-procedures">ยง4.3.1</a>).</li>
<li>After every use of these scripts, the Security Officer will verify their accuracy by reconciling their output with recent changes to production systems. The Security Officer will address any discrepancies immediately with changes to the scripts.</li>
</ul></li>
<li>All frontend functionality (developer dashboards and portals) is separated from backend (database and app servers) systems by being deployed on separate servers or containers.</li>
<li>All software and systems are tested using unit tests and end to end tests.</li>
<li>All committed code is reviewed using pull requests to assure software code quality and proactively detect potential security issues in development.</li>
<li>Onyx utilizes development and staging environments that mirror production to assure proper function.</li>
<li>Onyx also deploys environments locally using Vagrant to assure functionality before moving to staging or production.</li>
<li>All formal change requests require unique ID and authentication.</li>
<li>Onyx uses the <a href="http://iase.disa.mil/stigs/">Security Technical Implementation Guides (STIGs)</a> published by the Defense Information Systems Agency as a baseline for hardening systems.

<ul>
<li>Windows-based systems use a baseline Active Directory group policy configuration in conjunction with the Windows Server 2012 STIG.</li>
<li>Linux-based systems use a Red Hat Enterprise Linux STIG which has been adapted for Ubuntu and improved based on the results of subsequent vulnerability scans and risk assessments.</li>
</ul></li>
<li>Clocks are continuously synchronized to an authoritative source across all systems using NTP or a platform-specific equivalent. Modifying time data on systems is restricted.</li>
</ol>

<h2 id="9-3-provisioning-production-systems">9.3 Provisioning Production Systems</h2>

<ol>
<li>Before provisioning any systems, ops team members must file a request in the Onyx Quality Management System.

<ul>
<li>Quality Management System access requires authenticated users.</li>
<li>The CTO grants access to the Quality Management System following the procedures covered in the <a href="#7.2-access-establishment-and-modification">Access Establishment and Modification section</a>.</li>
</ul></li>
<li>The CTO, or an authorized delegate of the CTO, must approve the provisioning request before any new system can be provisioned.</li>
<li>Once provisioning has been approved, the ops team member must configure the new system according to the standard baseline chosen for the system&rsquo;s role.

<ul>
<li>For Linux systems, this means adding the appropriate grains to the Salt configuration file and running a <code>highstate</code> operation.</li>
<li>For Windows systems, this means adding the appropriate roles to the system&rsquo;s Chef profile and forcing a Chef run.</li>
</ul></li>
<li>If the system will be used to house production data (ePHI), the ops team member must add an encrypted block data volume to the VM during provisioning.

<ul>
<li>For systems on AWS, the ops team member must add an encrypted Elastic Block Storage (EBS) volume.</li>
<li>For systems on other cloud providers, the ops team member must add a block data volume and set up OS-level data encryption using Salt or Chef.</li>
</ul></li>
<li>Once the system has been provisioned, the ops team member must contact the security team to inspect the new system. A member of the security team will verify that the secure baseline has been applied to the new system, including (but not limited to) verifying the following items:

<ul>
<li>Removal of default users used during provisioning.</li>
<li>Network configuration for system.</li>
<li>Data volume encryption settings.</li>
<li>Intrusion detection and virus scanning software installed.</li>
<li>All items listed below in the operating system-specific subsections below.</li>
</ul></li>
<li>Once the security team member has verified the new system is correctly configured, the team member must add that system to the Nessus security scanner configuration.</li>
<li>The new system may be rotated into production once the CTO verifies all the provisioning steps listed above have been correctly followed and has marked the Issue with the <code>Approved</code> state.</li>
</ol>

<h3 id="9-3-1-provisioning-linux-systems">9.3.1 Provisioning Linux Systems</h3>

<ol>
<li>Linux systems have their baseline security configuration applied via Salt states. These baseline Salt states cover:

<ul>
<li>Ensuring that the machine is up-to-date with security patches and is configured to apply patches in accordance with our policies.</li>
<li>Stopping and disabling any unnecessary OS services.</li>
<li>Installing and configuring the OSSEC IDS agent.</li>
<li>Configuring 15-minute session inactivity timeouts.</li>
<li>Installing and configuring the ClamAV virus scanner.</li>
<li>Installing and configuring the NTP daemon, including ensuring that modifying system time cannot be performed by unprivileged users.</li>
<li>Configuring LUKS volumes for providers that do not have native support for encrypted data volumes, including ensuring that encryption keys are protected from unauthorized access.</li>
<li>Configuring authentication to the centralized LDAP servers.</li>
<li>Configuring audit logging as described in the <a href="#8.-auditing-policy">Auditing Policy section</a>.</li>
</ul></li>
<li>Any additional Salt states applied to the Linux system must be clearly documented by the ops team member in the DT request by specifying the purpose of the new system.</li>
</ol>

<h3 id="9-3-2-provisioning-windows-systems">9.3.2 Provisioning Windows Systems</h3>

<ol>
<li>Windows systems have their baseline security configuration applied via the combination of Group Policy settings and Chef recipes. These baseline settings cover:

<ul>
<li>Joining the Windows Domain Controller and applying the Active Directory Group Policy configuration.</li>
<li>Ensuring that the machine is up-to-date with security patches and is configured to apply patches in accordance with our policies.</li>
<li>Stopping and disabling any unnecessary OS services.</li>
<li>Installing and configuring the OSSEC IDS agent.</li>
<li>Configuring 15-minute session inactivity timeouts.</li>
<li>Installing and configuring the Avast virus scanner.</li>
<li>Configuring transport encryption according to the requirements described in <a href="#17.9-transmission-security">ยง17.9</a>.</li>
<li>Configuring the system clock, including ensuring that modifying system time cannot be performed by unprivileged users.</li>
<li>Configuring audit logging as described in the <a href="#8.-auditing-policy">Auditing Policy section</a>.</li>
</ul></li>
<li>Any additional Salt states applied to the Linux system must be clearly documented by the ops team member in the DT request by specifying the purpose of the new system.</li>
</ol>

<h3 id="9-3-3-provisioning-management-systems">9.3.3 Provisioning Management Systems</h3>

<ol>
<li>Provisioning management systems such as Salt servers, LDAP servers, or VPN appliances follows the same procedure as provisioning a production system.</li>
<li>Provisioning the first Salt server for a production pod requires bootstrapping Salt. An authorized member of the Dev Ops team will oversee provisioning a new Salt server.

<ul>
<li>Once the Salt server has been bootstrapped, the ops team member will apply the baseline configuration to the Salt server by performing a <code>highstate</code> operation as usual.</li>
</ul></li>
<li>Critical infrastructure services such as logging, monitoring, LDAP servers, or Windows Domain Controllers must be configured with appropriate Salt states.

<ul>
<li>These Salt states have been approved by the CTO, or an authorized delegate of the CTO, to be in accordance with all Onyx policies, including setting appropriate:

<ul>
<li>Audit logging requirements.</li>
<li>Password size, strength, and expiration requirements.</li>
<li>Transmission encryption requirements.</li>
<li>Network connectivity timeouts.</li>
</ul></li>
</ul></li>
<li>Critical infrastruture roles applied to new systems must be clearly documented by the ops team member in the DT request.</li>
</ol>

<h2 id="9-4-changing-existing-systems">9.4 Changing Existing Systems</h2>

<ol>
<li>Subsequent changes to already-provisioned systems are unconditionally handled by one of the following methods:

<ul>
<li>Changes to Salt states or pillar values.</li>
<li>Changes to Chef recipes.</li>
<li>For configuration changes that cannot be handled by Chef or Salt, a runbook describing exactly what changes will be made and by whom.</li>
</ul></li>
<li>Configuration changes to Chef recipes or Salt states must be initiated by creating a Merge Request in GitLab.

<ul>
<li>The ops team member will create a feature branch and make their changes on that branch.</li>
<li>The ops team member must test their configuration change locally when possible, or on a development and/or staging sandbox otherwise.</li>
<li>At least one other ops team member must review the Chef or Salt change before merging the change into the main branch.</li>
</ul></li>
<li>In all cases, before rolling out the change to production, the ops team member must file an Issue in the DT project describing the change. This Issue must link to the reviewed Merge Request and/or include a link to the runbook.</li>
<li>Once the request has been approved by the CTO, the ops team member may roll out the change into production environments.</li>
</ol>

<h2 id="9-5-patch-management-procedures">9.5 Patch Management Procedures</h2>

<ol>
<li>Onyx uses automated tooling to ensure systems are up-to-date with the latest security patches.</li>
<li>On Ubuntu Linux systems, the unattended-upgrades tool is used to apply security patches in phases.

<ul>
<li>The security team maintains a mirrored snapshot of security patches from the upstream OS vendor. This mirror is synchronized bi-weekly and applied to development systems nightly.</li>
<li>If the development systems function properly after the two-week testing period, the security team will promote that snapshot into the mirror used by all staging systems. These patches will be applied to all staging systems during the next nightly patch run.</li>
<li>If the staging systems function properly after the two-week testing period, the security team will promote that snapshot into the mirror used by all production systems. These patches will be applied to all production systems during the next nightly patch run.</li>
<li>Patches for critical kernel security vulnerabilities may be applied to production systems using hot-patching tools at the discretion of the Security Officer. These patches must follow the same phased testing process used for non-kernel security patches; this process may be expedited for severe vulnerabilities.</li>
</ul></li>
<li>On Windows systems, the baseline Group Policy setting configures Windows Update to implement the patching policy.</li>
</ol>

<h2 id="9-6-software-development-procedures">9.6 Software Development Procedures</h2>

<ol>
<li>All development uses feature branches based on the main branch used for the current release. Any changes required for a new feature or defect fix are committed to that feature branch.

<ul>
<li>These changes must be covered under 1) a unit test where possible, or 2) integration tests.</li>
<li>Integration tests are <em>required</em> if unit tests cannot reliably exercise all facets of the change.</li>
</ul></li>
<li>Developers are strongly encouraged to follow the <a href="https://github.com/blog/926-shiny-new-commit-styles">commit message conventions suggested by GitHub</a>.

<ul>
<li>Commit messages should be wrapped to 72 characters.</li>
<li>Commit messages should be written in the present tense. This convention matches up with commit messages generated by commands like git merge and git revert.</li>
</ul></li>
<li>Once the feature and corresponding tests are complete, a pull request will be created using the GitHub/GitLab web interface. The pull request should indicate which feature or defect is being addressed and should provide a high-level description of the changes made.</li>
<li>Code reviews are performed as part of the pull request procedure. Once a change is ready for review, the author(s) will notify other engineers using an appropriate mechanism, typically via an <code>@channel</code> message in Slack.

<ul>
<li>Other engineers will review the changes, using the guidelines above.</li>
<li>Engineers should note all potential issues with the code; it is the responsibility of the author(s) to address those issues or explain why they are not applicable.</li>
</ul></li>
<li>If the feature or defect interacts with ePHI, or controls access to data potentially containing ePHI, the code changes must be reviewed by two members of Onyx&rsquo;s Blue Team (software security team) before the feature is marked as complete.

<ul>
<li>The Blue Team members will provide a security analysis of features to ensure they satisfy Onyx&rsquo;s compliance and security commitments.</li>
<li>This review must include a security analysis for potential vulnerabilities such as those listed in the <a href="https://www.owasp.org/index.php/Top10">OWASP Top 10</a> or the <a href="http://cwe.mitre.org/top25/">CWE top 25</a>.</li>
<li>This review must also verify that any actions performed by authenticated users will generate appropriate audit log entries.</li>
<li>Blue Team members are required to undergo annual training on identifying the most common software vulnerabilities and will receive ongoing training on Onyx&rsquo;s compliance and security requirements.</li>
</ul></li>
<li>Once the review process finishes, each reviewer should leave a comment on the pull request saying &ldquo;looks good to me&rdquo; (often abbreviated as &ldquo;LGTM&rdquo;), at which point the original author(s) may merge their change into the release branch.</li>
</ol>

<h2 id="9-7-software-release-procedures">9.7 Software Release Procedures</h2>

<ol>
<li>Software releases are treated as changes to existing systems and thus follow the procedure described in <a href="#9.4-changing-existing-systems">ยง9.4</a>.</li>
</ol>
